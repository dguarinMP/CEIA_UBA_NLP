{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# **Procesamiento de Lenguaje Natural**\n",
    "# **Desafío 3: Modelo de lenguaje con tokenización por caracteres**\n",
    "\n",
    "> **Carrera de Especialización en Inteligencia Artificial, Facultad de Ingeniería**\n",
    ">\n",
    "> **Universidad de Buenos Aires, Junio de 2024**\n",
    ">\n",
    "> Edgar David Guarin Castro (davidg@marketpsychdata.com)\n",
    "\n",
    "En el presente trabajo se usa el dataset de [notícias financieras de Reuters](https://github.com/duynht/financial-news-dataset) con el fin de:\n",
    "- realizar un pre-procesamiento para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determinístico y estocástico. En este último caso se busca observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "## **0. Importando librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Input, TimeDistributed, CategoryEncoding, SimpleRNN\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Cargando los datos**\n",
    "\n",
    "El dataset de noticias financieras contiene 109,110 noticias de Reuters, recopiladas y utilizadas por primera vez en una investigación de [Ding et al. (2014)](https://emnlp2014.org/papers/pdf/EMNLP2014148.pdf). El dataset se ha utilizado en otros estudios de predicción de movimientos del precio de acciones basados en eventos estructurados.\n",
    "\n",
    "Despues de clonar el repositório, una carpeta llamada *financial-news-dataset* con todos los artículos, es creada en el directorio local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'financial-news-dataset'...\n",
      "remote: Enumerating objects: 109112, done.\u001b[K\n",
      "remote: Total 109112 (delta 0), reused 0 (delta 0), pack-reused 109112\u001b[K\n",
      "Receiving objects: 100% (109112/109112), 162.92 MiB | 16.14 MiB/s, done.\n",
      "Resolving deltas: 100% (1046/1046), done.\n",
      "Updating files: 100% (106523/106523), done.\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# Clonando el repositorio\n",
    "#-----------------\n",
    "!git clone https://github.com/duynht/financial-news-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los artículos del corpus son cargados a un dataframe. Se escogen los 100 primeros artículos para facilitar los cálculos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\u0000\u0000\u0000\u0001Bud1\u0000\u0000\u0010\u0000\u0000\u0000\b\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-- Hitachi, GE boost alliance in nuclear power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-- Volvo to cut 1,000 staff at Virginia plant\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-- European banks hiding full pension obligati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-- Hitachi, GE to form joint nuclear power ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-- Darden Restaurants sales mixed in March\\n--...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-- Gap to offer limited-edition designer colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-- KKR confirms dropped out of Sainsbury biddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-- Wal-Mart expands personal well-being progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-- U.S. cable TV group withdraws from eBay ad ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              article\n",
       "0   \u0000\u0000\u0000\u0001Bud1\u0000\u0000\u0010\u0000\u0000\u0000\b\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...\n",
       "1   -- Hitachi, GE boost alliance in nuclear power...\n",
       "2   -- Volvo to cut 1,000 staff at Virginia plant\\...\n",
       "3   -- European banks hiding full pension obligati...\n",
       "4   -- Hitachi, GE to form joint nuclear power ven...\n",
       "..                                                ...\n",
       "95  -- Darden Restaurants sales mixed in March\\n--...\n",
       "96  -- Gap to offer limited-edition designer colle...\n",
       "97  -- KKR confirms dropped out of Sainsbury biddi...\n",
       "98  -- Wal-Mart expands personal well-being progra...\n",
       "99  -- U.S. cable TV group withdraws from eBay ad ...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------\n",
    "# Función para leer todas las noticias\n",
    "#-----------------\n",
    "def read_news(directory):\n",
    "    news = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                news.append(f.read())\n",
    "    return news\n",
    "\n",
    "#-----------------\n",
    "# Leyendo las noticias del directorio ReutersNews106521\n",
    "#-----------------\n",
    "news = read_news('financial-news-dataset/ReutersNews106521')\n",
    "\n",
    "#-----------------\n",
    "# Creando un DataFrame con la mitad de las noticias\n",
    "#-----------------\n",
    "df = pd.DataFrame(news, columns=['article'])\n",
    "\n",
    "df = df[:100]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Creando el vocabulario**\n",
    "\n",
    "Con los artículos seleccionados, se escoge el tamaño del contexto y se crea el vocabulario de caracteres seleccionando los caracteres únicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 97\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# Seleccionando el tamaño de contexto\n",
    "#-----------------\n",
    "max_context_size = 100\n",
    "\n",
    "#-----------------\n",
    "# Uniendo todos los artículos en un solo string\n",
    "#-----------------\n",
    "all_text = ' '.join(df['article'].tolist())\n",
    "\n",
    "#-----------------\n",
    "# Creando el conjunto de caracteres únicos\n",
    "#-----------------\n",
    "chars_vocab = sorted(set(all_text))\n",
    "\n",
    "vocab_size = len(chars_vocab)\n",
    "\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el vocabulario, se construyen los dicionarios que asignan índices a caracteres y viceversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens: 281475\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# El diccionario `char2idx` servirá como tokenizador\n",
    "#-----------------\n",
    "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
    "idx2char = {v: k for k,v in char2idx.items()}\n",
    "\n",
    "#-----------------\n",
    "# tokenizando el texto completo\n",
    "#-----------------\n",
    "tokenized_text = [char2idx[ch] for ch in all_text]\n",
    "print(f\"Número de tokens: {len(tokenized_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separa a continuación el dataset en un conjunto de entrenamiento y validación. La variable `p_val` guardará la proporción del corpus (10%) que se reservará para validación. La variable `num_val` guardará la cantidad de secuencias de tamaño `max_context_size` que se usará en la validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = 0.1\n",
    "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))\n",
    "\n",
    "#-----------------\n",
    "# Separando la porción de texto utilizada en entrenamiento de la de validación.\n",
    "#-----------------\n",
    "train_text = tokenized_text[:-num_val*max_context_size]\n",
    "val_text = tokenized_text[-num_val*max_context_size:]\n",
    "\n",
    "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]\n",
    "\n",
    "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]\n",
    "\n",
    "X = np.array(tokenized_sentences_train[:-1])\n",
    "y = np.array(tokenized_sentences_train[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Definiendo el modelo**\n",
    "\n",
    "Este modelo de lenguaje de caracteres está diseñado para predecir el siguiente carácter en una secuencia dada. La secuencia se codifica como one-hot y se procesa a través de una capa RNN, que captura dependencias temporales en los datos. Luego, una capa densa con activación softmax produce una distribución de probabilidad sobre el vocabulario para predecir el siguiente carácter en la secuencia.\n",
    "\n",
    "Para llevar a cabo este proceso se hace lo siguiente:\n",
    "- se inicializa el modelo con la función `Sequential` de Keras, que permite apilar capas de redes neuronales una sobre otra en orden secuencial.\n",
    "- se adiciona una capa `TimeDistributed` que aplica una capa `CategoryEncoding` en cada paso de tiempo de una secuencia. Esto asegura que la codificación se aplique a cada carácter en la secuencia independientemente. `CategoryEncoding` por su parte realiza la codificación one-hot de cada carácter en la secuencia. Dado que vocab_size representa el número total de caracteres únicos en el vocabulario, esta capa convierte cada carácter en un vector de longitud vocab_size donde un único elemento es 1 (el que corresponde al índice del carácter) y los demás son 0. `input_shape=(None,1)` especifica que la entrada tendrá una longitud variable (por eso el None) y cada carácter se representa inicialmente como un entero (por eso el 1).\n",
    "- se adiciona una capa recurrente simple (RNN), que procesa secuencia de caracteres. Esta capa tiene 200 unidades (o neuronas), lo que significa que la salida de esta capa será un vector de 200 dimensiones para cada carácter en la secuencia.\n",
    "- se adiciona una capa `Dense` completamente conectada donde cada uno de los 200 valores que vienen de la RNN se conectará a cada uno de los nodos `vocab_size` en esta capa.\n",
    "- se compila el modelo usando `loss='sparse_categorical_crossentropy'` para implementar la función de pérdida sparse_categorical_crossentropy, que es adecuada cuando las etiquetas de destino son enteros como los índices de los caracteres. Esta función mide la diferencia entre las distribuciones de probabilidad predicha y la real. La función `optimizer='rmsprop'` es un optimizador usado para tareas de procesamiento de secuencias. Ajusta los pesos del modelo para minimizar la función de pérdida, y es adecuado para problemas de series temporales y secuencias debido a su capacidad de manejar el decaimiento adaptativo del aprendizaje.\n",
    "- Finalmente, se imprime un resumen del modelo que muestra la estructura de las capas, el número de parámetros (pesos) en cada capa, y la cantidad total de parámetros del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">59,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,497</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │        \u001b[38;5;34m59,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m)       │        \u001b[38;5;34m19,497\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,097</span> (308.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m79,097\u001b[0m (308.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,097</span> (308.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m79,097\u001b[0m (308.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-----------------\n",
    "# Inicializando el modelo\n",
    "#-----------------\n",
    "model = Sequential()\n",
    "\n",
    "#-----------------\n",
    "# Adicionando las capas\n",
    "#-----------------\n",
    "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
    "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "#-----------------\n",
    "# Imprimiendo la estructura del modelo\n",
    "#-----------------\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular la perplejidad en el conjunto de datos de validación al final de cada época, se define la clase `PplCallback`. Esta clase también aplica early stopping si el rendimiento no mejora después de un número específico de épocas. Esto ayuda a asegurar que el modelo no se sobreajuste y que se mantenga la mejor versión del modelo entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PplCallback(keras.callbacks.Callback):\n",
    "\n",
    "    '''\n",
    "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
    "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
    "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
    "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
    "    si la perplejidad no mejora después de `patience` epochs.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, val_data, history_ppl,patience=5):\n",
    "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
    "      # mediremos la perplejidad\n",
    "      self.val_data = val_data\n",
    "\n",
    "      self.target = []\n",
    "      self.padded = []\n",
    "\n",
    "      count = 0\n",
    "      self.info = []\n",
    "      self.min_score = np.inf\n",
    "      self.patience_counter = 0\n",
    "      self.patience = patience\n",
    "\n",
    "      # nos movemos en todas las secuencias de los datos de validación\n",
    "      for seq in self.val_data:\n",
    "\n",
    "        len_seq = len(seq)\n",
    "        # armamos todas las subsecuencias\n",
    "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
    "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
    "\n",
    "        if len(subseq)!=0:\n",
    "\n",
    "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
    "\n",
    "          self.info.append((count,count+len_seq))\n",
    "          count += len_seq\n",
    "\n",
    "      self.padded = np.vstack(self.padded)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
    "        scores = []\n",
    "\n",
    "        predictions = self.model.predict(self.padded,verbose=0)\n",
    "\n",
    "        # para cada secuencia de validación\n",
    "        for start,end in self.info:\n",
    "\n",
    "          # en `probs` iremos guardando las probabilidades de los términos target\n",
    "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
    "\n",
    "          # calculamos la perplejidad por medio de logaritmos\n",
    "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
    "\n",
    "        # promediamos todos los scores e imprimimos el valor promedio\n",
    "        current_score = np.mean(scores)\n",
    "        history_ppl.append(current_score)\n",
    "        print(f'\\n mean perplexity: {current_score} \\n')\n",
    "\n",
    "        # chequeamos si tenemos que detener el entrenamiento\n",
    "        if current_score < self.min_score:\n",
    "          self.min_score = current_score\n",
    "          self.model.save(\"my_model.keras\")\n",
    "          print(\"Saved new model!\")\n",
    "          self.patience_counter = 0\n",
    "        else:\n",
    "          self.patience_counter += 1\n",
    "          if self.patience_counter == self.patience:\n",
    "            print(\"Stopping training...\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Entrenamiento**\n",
    "\n",
    "A continuación se entrena el modelo de lenguaje de caracteres y al mismo tiempo se monitorea la perplejidad en el conjunto de validación para evaluar el rendimiento del modelo. Después del entrenamiento, se grafica la evolución de la perplejidad a lo largo de las épocas para visualizar cómo ha mejorado (o no) el modelo. Esta gráfica es crucial para entender si el modelo se está ajustando bien a los datos o si se está produciendo sobreajuste o estancamiento.\n",
    "\n",
    "Durante el ajuste, se agrega un `callback` para evaluar la perplejidad del modelo en el conjunto de validación al final de cada época. La perplejidad se guarda en la lista `history_ppl` y se implementa early stopping si la perplejidad no mejora después de un número definido de épocas (patience). El `batch_size` se puede seleccionar a mano y en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 2.1278\n",
      " mean perplexity: 7.624757953679742 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 223ms/step - loss: 2.1277\n",
      "Epoch 2/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 1.9553\n",
      " mean perplexity: 6.735830078387543 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 197ms/step - loss: 1.9553\n",
      "Epoch 3/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.8643\n",
      " mean perplexity: 6.44209626109817 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 196ms/step - loss: 1.8643\n",
      "Epoch 4/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1.8103\n",
      " mean perplexity: 6.09653876836855 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 205ms/step - loss: 1.8103\n",
      "Epoch 5/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.7762\n",
      " mean perplexity: 5.995613625468228 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 196ms/step - loss: 1.7762\n",
      "Epoch 6/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 1.7517\n",
      " mean perplexity: 5.933817868621863 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 198ms/step - loss: 1.7517\n",
      "Epoch 7/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 1.7351\n",
      " mean perplexity: 5.9370881382079945 \n",
      "\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 199ms/step - loss: 1.7351\n",
      "Epoch 8/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.7198\n",
      " mean perplexity: 5.851597766204319 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 196ms/step - loss: 1.7198\n",
      "Epoch 9/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 1.7070\n",
      " mean perplexity: 5.670591831052152 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 189ms/step - loss: 1.7070\n",
      "Epoch 10/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 1.6989\n",
      " mean perplexity: 5.652121705755468 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 193ms/step - loss: 1.6989\n",
      "Epoch 11/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 1.6898\n",
      " mean perplexity: 5.6418041330275575 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 188ms/step - loss: 1.6898\n",
      "Epoch 12/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 1.6814\n",
      " mean perplexity: 5.583867492721138 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 199ms/step - loss: 1.6814\n",
      "Epoch 13/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 1.6755\n",
      " mean perplexity: 5.552583720048095 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 185ms/step - loss: 1.6755\n",
      "Epoch 14/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 1.6717\n",
      " mean perplexity: 5.560743546575079 \n",
      "\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 192ms/step - loss: 1.6717\n",
      "Epoch 15/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 1.6664\n",
      " mean perplexity: 5.6244533667414185 \n",
      "\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 192ms/step - loss: 1.6664\n",
      "Epoch 16/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 1.6610\n",
      " mean perplexity: 5.551470991244768 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 186ms/step - loss: 1.6610\n",
      "Epoch 17/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 1.6568\n",
      " mean perplexity: 5.502057455203697 \n",
      "\n",
      "Saved new model!\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 190ms/step - loss: 1.6568\n",
      "Epoch 18/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1.6521\n",
      " mean perplexity: 5.6323807967097785 \n",
      "\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 202ms/step - loss: 1.6521\n",
      "Epoch 19/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 1.6479\n",
      " mean perplexity: 5.568140027355129 \n",
      "\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 191ms/step - loss: 1.6479\n",
      "Epoch 20/20\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1.6455\n",
      " mean perplexity: 5.5256152719231055 \n",
      "\n",
      "\u001b[1m989/989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 202ms/step - loss: 1.6455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-----------------\n",
    "# Lista para almacenar los valores de perplejidad calculados al final de cada época\n",
    "#-----------------\n",
    "history_ppl = []\n",
    "\n",
    "#-----------------\n",
    "# Ajuste del modelo\n",
    "#-----------------\n",
    "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=256)\n",
    "\n",
    "#-----------------\n",
    "# Entrenamiento\n",
    "#-----------------\n",
    "epoch_count = range(1, len(history_ppl) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico anterior de perplejidad en función de las épocas muestra que:\n",
    "- la perplejidad disminuye a medida que avanza el entrenamiento, lo que indica que el modelo está mejorando en su capacidad para predecir las secuencias de caracteres en el conjunto de validación.\n",
    "- el aprendizaje es más rápido durante las primeras épocas. Esto es típico en muchos modelos de aprendizaje profundo, donde las mayores ganancias se producen al principio.\n",
    "- después de la décima época, la perplejidad comienza a estabilizarse con algunas fluctuaciones. Esto podría indicar que el modelo está llegando a un punto de saturación donde las mejoras adicionales son menores. Las fluctuaciones podrían deberse a variaciones naturales en los datos de validación o a pequeños ajustes en los pesos del modelo.\n",
    "- no hay aumento significativo de la perplejidad en las últimas épocas, lo que sugiere que el modelo no está sobreajustando los datos de entrenamiento. El early stopping no fue necesario en este caso, ya que el modelo continuó mejorando o manteniéndose estable.\n",
    "\n",
    "Por lo anterior, se deduce que el modelo mejoró constantemente, mostrando un entrenamiento exitoso, lo que sugiere que sea efectivo para la tarea de predicción de secuencias de caracteres.\n",
    "\n",
    "Así, se carga el mejor modelo que será usado para hacer las inferencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------\n",
    "# Cargando el mejor modelo\n",
    "#-----------------\n",
    "model = keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Generación de caracteres**\n",
    "\n",
    "En la siguiente celda se implementa una interfaz interactiva y simple, que permite probar fácilmente cómo el modelo de lenguaje basado en caracteres predice continuaciones de texto. La interfaz es creada con Gradio, una biblioteca que permite crear interfaces web simples para modelos de machine learning.\n",
    "\n",
    "El usuario ingresa un texto en el cuadro de texto `human_text` y el modelo generará un carácter adicional que es la predicción más probable de cuál sería el siguiente carácter en la secuencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_response(human_text):\n",
    "\n",
    "    #-----------------\n",
    "    # Codificación del texto\n",
    "    #-----------------\n",
    "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
    "\n",
    "    #-----------------\n",
    "    # Asegurando que la secuencia codificada tenga la longitud el tamaño del contexto\n",
    "    #-----------------\n",
    "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
    "\n",
    "    #-----------------\n",
    "    # Predicción softmax\n",
    "    #-----------------\n",
    "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
    "\n",
    "    #-----------------\n",
    "    # Debemos buscar en el vocabulario el caracter\n",
    "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "    #-----------------\n",
    "    out_word = ''\n",
    "    out_word = idx2char[y_hat]\n",
    "\n",
    "    #-----------------\n",
    "    # Agrego la palabra a la frase predicha\n",
    "    #-----------------\n",
    "    return human_text + out_word\n",
    "\n",
    "\n",
    "#-----------------\n",
    "# Interfaz de Gradio\n",
    "#-----------------\n",
    "iface = gr.Interface(\n",
    "    fn=model_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=\"text\")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Generación de secuencias por Greedy Search**\n",
    "\n",
    "La metodología Greedy Search (búsqueda codiciosa) es una estrategia simple para crear secuencias en tareas de generación de texto. En esta metodología se selecciona el token (o carácter) que tenga la mayor probabilidad predicha por el modelo en ese momento. Es un enfoque \"codicioso\" porque siempre elige la opción que parece ser la mejor en el momento, sin considerar las posibles mejores secuencias que podrían surgir al explorar otras opciones con menores probabilidades.\n",
    "\n",
    "La función `generate_seq` implementa este método tomando un texto inicial y generando una secuencia extendida de texto al predecir un carácter a la vez, utilizando el modelo de caracteres escogido. En cada iteración, codifica el texto actual, predice el siguiente carácter, lo decodifica, y lo agrega al texto. Este proceso se repite hasta que se ha generado el número deseado de caracteres adicionales (`n_words`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\n",
    "    #-----------------\n",
    "\t# Generando un numero fijo de palabras\n",
    "    #-----------------\n",
    "    for _ in range(n_words):\n",
    "        #-----------------\n",
    "\t\t# Codificando el texto\n",
    "        #-----------------\n",
    "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
    "\n",
    "        #-----------------\n",
    "\t\t# Padding de la secuencia\n",
    "        #-----------------\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "\t\t#-----------------\n",
    "        # Predicción softmax\n",
    "        #-----------------\n",
    "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
    "\n",
    "        #-----------------\n",
    "\t\t# Concatenando las predicciones\n",
    "        #-----------------\n",
    "        out_word = ''\n",
    "\n",
    "        out_word = idx2char[y_hat]\n",
    "\n",
    "        #-----------------\n",
    "\t\t# Agregando las palabras a la frase predicha\n",
    "        #-----------------\n",
    "        output_text += out_word\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior se implementa ahora con un ejemplo de texto con términos financieros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the company reported high earnings during the last year and the bank of the bank of the bank of the '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='the company reported high earnings during the last'\n",
    "\n",
    "generate_seq(model, input_text, max_length=max_context_size, n_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuantas más palabras se agreguen al texto de entrada, mejor es la predicción de las primeras palabras generadas. Sin embargo, al aumentar `n_words` la calidad de la predicción de las últimas palabras generadas disminuye y el texto pierde sentido, al volverse repetitivo como se observa en el caso anterior.\n",
    "\n",
    "Esto se debe a que los modelos RNN estándar tienen una capacidad limitada para recordar información a lo largo de secuencias largas. En otras palabras, después de un cierto número de caracteres, el modelo puede \"olvidar\" el contexto original y comenzar a repetir patrones aprendidos.\n",
    "\n",
    "Además, el modelo ha sido entrenado en un conjunto de datos reducido donde ciertas frases o patrones se repiten con frecuencia. Como consecuencia, el modelo aprende a repetir esas secuencias cuando no está seguro de qué generar a continuación, llevando a predicciones muy determinísticas y repetitivas.\n",
    "\n",
    "## **7. Generación de secuencias por Beam Search**\n",
    "\n",
    "Beam Search es una estrategia más avanzada que Greedy Search para la generación de secuencias pues este método explora múltiples caminos simultáneamente para encontrar la secuencia más probable de una manera más global.\n",
    "\n",
    "Beam Search mantiene un conjunto de los k mejores caminos (secuencias parciales) en cada paso. Este conjunto de caminos se conoce como el \"beam\" o haz. El parámetro k determina cuántos caminos se exploran en paralelo. Un k mayor significa que más opciones se consideran, lo que generalmente lleva a secuencias de mayor calidad, pero también requiere más memoria y tiempo de cómputo.\n",
    "\n",
    "En cada paso, el modelo predice las probabilidades para todos los posibles caracteres siguientes. Cada camino en el beam se expande con todos estos posibles caracteres, generando nuevas secuencias. Luego, se seleccionan las k secuencias con mayor probabilidad acumulada (considerando la probabilidad total de cada secuencia hasta el momento) para continuar con el siguiente paso. Las secuencias menos probables se podan o eliminan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------\n",
    "# funcionalidades para hacer encoding y decoding\n",
    "#-----------------\n",
    "def encode(text,max_length=max_context_size):\n",
    "\n",
    "    encoded = [char2idx[ch] for ch in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    return ''.join([idx2char[ch] for ch in seq if ch != 0])\n",
    "\n",
    "#-----------------\n",
    "# función que selecciona candidatos para el beam search\n",
    "#-----------------\n",
    "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
    "  #-----------------\n",
    "  # colectando todas las probabilidades para la siguiente búsqueda\n",
    "  #-----------------\n",
    "  pred_large = []\n",
    "\n",
    "  for idx,pp in enumerate(pred):\n",
    "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
    "\n",
    "  pred_large = np.array(pred_large)\n",
    "  \n",
    "  #-----------------\n",
    "  # criterio de selección\n",
    "  #-----------------\n",
    "  if mode == 'det':\n",
    "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
    "  elif mode == 'sto':\n",
    "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
    "  else:\n",
    "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
    "\n",
    "  #-----------------\n",
    "  # traducir a índices de token en el vocabulario\n",
    "  #-----------------\n",
    "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
    "                        np.array([idx_select%vocab_size]).T),\n",
    "                      axis=1)\n",
    "\n",
    "  #-----------------\n",
    "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "  #-----------------\n",
    "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "#-----------------\n",
    "# Implementando beam search\n",
    "#-----------------\n",
    "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
    "\n",
    "    #-----------------\n",
    "    # codificando el texto\n",
    "    #-----------------\n",
    "    encoded = encode(input)\n",
    "\n",
    "    #-----------------\n",
    "    # Primera prediccion\n",
    "    #-----------------\n",
    "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
    "\n",
    "    #-----------------\n",
    "    # obteniendo tamaño del vocabulario\n",
    "    #-----------------\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    #-----------------\n",
    "    # Inicializando la historia\n",
    "    #-----------------\n",
    "    history_probs = [0]*num_beams\n",
    "    history_tokens = [encoded[0]]*num_beams\n",
    "\n",
    "    #-----------------\n",
    "    # seleccionando los num_beams candidatos\n",
    "    #-----------------\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                        num_beams,\n",
    "                                        vocab_size,\n",
    "                                        history_probs,\n",
    "                                        history_tokens,\n",
    "                                        temp,\n",
    "                                        mode)\n",
    "\n",
    "    #-----------------\n",
    "    # Ciclo de busqueda del beam\n",
    "    #-----------------\n",
    "    for i in range(num_words-1):\n",
    "\n",
    "      preds = []\n",
    "\n",
    "      for hist in history_tokens:\n",
    "\n",
    "        #-----------------\n",
    "        # actualizando la secuencia de tokens\n",
    "        #-----------------\n",
    "        input_update = np.array([hist[i+1:]]).copy()\n",
    "\n",
    "        #-----------------\n",
    "        # predicción\n",
    "        #-----------------\n",
    "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
    "\n",
    "        preds.append(y_hat)\n",
    "\n",
    "      history_probs, history_tokens = select_candidates(preds,\n",
    "                                                        num_beams,\n",
    "                                                        vocab_size,\n",
    "                                                        history_probs,\n",
    "                                                        history_tokens,\n",
    "                                                        temp,\n",
    "                                                        mode)\n",
    "\n",
    "    return history_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1 Predicción con Beam Search determinístico**\n",
    "\n",
    "En el siguiente ejemplo se usa el texto introducido anteriormente para el caso de Greedy Search, pero esta vez se genera una secuencia de caracteres con un Beam Search determinístico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto decodificado 1:\n",
      "the company reported high earnings during the last year. The company's financial company's financial\n",
      "\n",
      "Texto decodificado 2:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company's \n",
      "\n",
      "Texto decodificado 3:\n",
      "the company reported high earnings during the last year. The company's financial reporting by  March\n",
      "\n",
      "Texto decodificado 4:\n",
      "the company reported high earnings during the last year. The company's financial reporting by  Monda\n",
      "\n",
      "Texto decodificado 5:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company, w\n",
      "\n",
      "Texto decodificado 6:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company wi\n",
      "\n",
      "Texto decodificado 7:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company sa\n",
      "\n",
      "Texto decodificado 8:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company co\n",
      "\n",
      "Texto decodificado 9:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company se\n",
      "\n",
      "Texto decodificado 10:\n",
      "the company reported high earnings during the last year. The company's first quarter. The company fo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# predicción con beam search\n",
    "#-----------------\n",
    "salidas = beam_search(model,num_beams=10,num_words=50,input=input_text)\n",
    "\n",
    "#-----------------\n",
    "# Mostrando las salidas\n",
    "#-----------------\n",
    "for i, encoded_seq in enumerate(salidas):\n",
    "    decoded_text = decode(encoded_seq)\n",
    "    print(f\"Texto decodificado {i+1}:\\n{decoded_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, la selección es completamente determinista y no depende de la temperatura. Por lo tanto, siempre se seleccionan las secuencias con las mayores probabilidades calculadas, lo que resulta en las mismas salidas cada vez que se ejecuta el método.\n",
    "\n",
    "### **7.2 Predicción con Beam Search estocástico**\n",
    "\n",
    "El modo estocástico de Beam Search introduce aleatoriedad en el proceso de selección de secuencias, permitiendo que se exploren diferentes rutas de generación de texto en lugar de siempre seleccionar las opciones más probables de manera determinista. Esto puede llevar a secuencias más variadas y menos predecibles.\n",
    "\n",
    "Aquí, la temperatura afecta la aleatoriedad de la selección, y por lo tanto, el resultado puede variar en cada ejecución, incluso si la entrada es la misma.\n",
    "\n",
    "Por ejemplo, si la temperatura es 1, entonces la distribución de probabilidades no será afectada y el softmax aplicado será estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto decodificado 1:\n",
      "the company reported high earnings during the last year. The company said in the company will percen\n",
      "\n",
      "Texto decodificado 2:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n",
      "Texto decodificado 3:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n",
      "Texto decodificado 4:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n",
      "Texto decodificado 5:\n",
      "the company reported high earnings during the last year. The company said in the company will percen\n",
      "\n",
      "Texto decodificado 6:\n",
      "the company reported high earnings during the last year. The company said in the company will percen\n",
      "\n",
      "Texto decodificado 7:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n",
      "Texto decodificado 8:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n",
      "Texto decodificado 9:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n",
      "Texto decodificado 10:\n",
      "the company reported high earnings during the last year. The company said in the company will of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# predicción con beam search\n",
    "#-----------------\n",
    "salidas = beam_search(model,num_beams=10,num_words=50,input=input_text,temp=1,mode='sto')\n",
    "\n",
    "#-----------------\n",
    "# Mostrando las salidas\n",
    "#-----------------\n",
    "for i, encoded_seq in enumerate(salidas):\n",
    "    decoded_text = decode(encoded_seq)\n",
    "    print(f\"Texto decodificado {i+1}:\\n{decoded_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la temperatura es menor que 1, la distribución es más aguda, lo que refuerza las diferencias en las probabilidades. Las opciones más probables se vuelven aún más probables, y las opciones menos probables tienen menos chance de ser seleccionadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto decodificado 1:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 2:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 3:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 4:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 5:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 6:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 7:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 8:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 9:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n",
      "Texto decodificado 10:\n",
      "the company reported high earnings during the last year and the bank of the company said the company\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# predicción con beam search\n",
    "#-----------------\n",
    "salidas = beam_search(model,num_beams=10,num_words=50,input=input_text,temp=0.1,mode='sto')\n",
    "\n",
    "#-----------------\n",
    "# Mostrando las salidas\n",
    "#-----------------\n",
    "for i, encoded_seq in enumerate(salidas):\n",
    "    decoded_text = decode(encoded_seq)\n",
    "    print(f\"Texto decodificado {i+1}:\\n{decoded_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la temperatura es mayor que 1, las probabilidades se distribuyen más uniformemente. Esto significa que incluso las opciones con probabilidades más bajas tienen una mayor posibilidad de ser seleccionadas, aumentando el riesgo de pérdida de contexto y sentido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto decodificado 1:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition, mp\n",
      "\n",
      "Texto decodificado 2:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponitial wit\n",
      "\n",
      "Texto decodificado 3:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition,  q\n",
      "\n",
      "Texto decodificado 4:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition, tr\n",
      "\n",
      "Texto decodificado 5:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponitial wil\n",
      "\n",
      "Texto decodificado 6:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition,  s\n",
      "\n",
      "Texto decodificado 7:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponitial wiv\n",
      "\n",
      "Texto decodificado 8:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition, th\n",
      "\n",
      "Texto decodificado 9:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition, my\n",
      "\n",
      "Texto decodificado 10:\n",
      "the company reported high earnings during the last matter with Tosa, ained with To a remponition,  p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# predicción con beam search\n",
    "#-----------------\n",
    "salidas = beam_search(model,num_beams=10,num_words=50,input=input_text,temp=3,mode='sto')\n",
    "\n",
    "#-----------------\n",
    "# Mostrando las salidas\n",
    "#-----------------\n",
    "for i, encoded_seq in enumerate(salidas):\n",
    "    decoded_text = decode(encoded_seq)\n",
    "    print(f\"Texto decodificado {i+1}:\\n{decoded_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
